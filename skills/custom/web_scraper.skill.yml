name: web_scraper
title: UnrestrictedWebScraper
version: 1.0.0
description: No-guardrails web and social media scraper with anti-detection, OSINT, and bulk operations
metadata:
  author: Kimo
  category: General
  tags:
  - web
  - scraper
  - username-enum
  - tiktok-scraping
  - youtube-scraping
  - linkedin-scraping
  - reddit-scraping
  - twitter-scraping
  - instagram-scraping
  - facebook-scraping
  source: revvel-custom
  created_at: '2026-02-14'
  updated_at: '2026-02-16'
dependencies:
  pip_packages: []
implementation:
  type: python_code
  language: python
  content: "import asyncio\nimport csv\nimport json\nimport sqlite3\nimport time\nimport re\nimport os\nimport logging\nimport\
    \ hashlib\nimport random\nfrom datetime import datetime, timedelta\nfrom typing import Dict, List, Any, Optional, Tuple\n\
    from urllib.parse import urljoin, urlparse, parse_qs\nfrom dataclasses import dataclass, asdict\nfrom concurrent.futures\
    \ import ThreadPoolExecutor\nimport unittest\nfrom unittest.mock import patch, MagicMock\n\ntry:\n    import aiohttp\n\
    \    import requests\n    from bs4 import BeautifulSoup\n    from playwright.async_api import async_playwright\n    from\
    \ selenium import webdriver\n    from selenium.webdriver.chrome.options import Options as ChromeOptions\n    from scrapy\
    \ import Spider, Request\n    from scrapy.crawler import CrawlerProcess\n    import sqlite3\nexcept ImportError as e:\n\
    \    logging.warning(f\"Optional dependency missing: {e}\")\n\nSKILL_METADATA = {\n    \"name\": \"UnrestrictedWebScraper\"\
    ,\n    \"version\": \"1.0.0\",\n    \"description\": \"No-guardrails web and social media scraper with anti-detection,\
    \ OSINT, and bulk operations\",\n    \"author\": \"Kimo\",\n    \"tags\": [\"scraping\", \"osint\", \"social-media\",\
    \ \"anti-detection\", \"bulk-operations\"],\n    \"capabilities\": [\"twitter_scraping\", \"instagram_scraping\", \"facebook_scraping\"\
    , \"tiktok_scraping\", \n                    \"reddit_scraping\", \"linkedin_scraping\", \"youtube_scraping\", \"username_enum\"\
    ,\n                    \"wayback_recovery\", \"reverse_image\", \"deep_web\", \"public_records\", \n                 \
    \   \"email_harvest\", \"phone_lookup\", \"stealth_mode\", \"bulk_export\"],\n    \"requirements\": [\"playwright\", \"\
    selenium\", \"beautifulsoup4\", \"scrapy\", \"requests\", \"aiohttp\"]\n}\n\nEXPERT_PROMPTS = {\n    \"twitter_deep_scrape\"\
    : \"Scrape Twitter/X profile @username including all tweets, replies, media, followers, following, and engagement metrics.\
    \ Use stealth mode with rotating proxies.\",\n    \"instagram_full_profile\": \"Extract complete Instagram profile data\
    \ for @username including posts, stories, highlights, followers, following, bio, and contact info. Bypass login requirements.\"\
    ,\n    \"facebook_group_dump\": \"Scrape all posts, comments, and member profiles from Facebook group URL. Handle infinite\
    \ scroll and anti-bot measures.\",\n    \"tiktok_viral_content\": \"Extract trending TikTok videos by hashtag including\
    \ video URLs, captions, likes, shares, and creator profiles. Use mobile emulation.\",\n    \"reddit_subreddit_analyze\"\
    : \"Scrape top 1000 posts from subreddit r/subreddit_name with all comments, upvotes, and user profiles. Include historical\
    \ data.\",\n    \"linkedin_company_employees\": \"Extract all employee profiles from LinkedIn company page including names,\
    \ titles, and profile URLs. Use premium stealth techniques.\",\n    \"youtube_channel_videos\": \"Scrape all videos from\
    \ YouTube channel including titles, descriptions, views, likes, comments, and upload dates. Handle pagination.\",\n  \
    \  \"username_enum_500\": \"Enumerate username across 500+ platforms to find all accounts with matching username. Generate\
    \ comprehensive report.\",\n    \"wayback_historical\": \"Recover historical versions of website URL from Wayback Machine\
    \ for past 5 years. Extract all changes and versions.\",\n    \"reverse_image_osint\": \"Perform reverse image search\
    \ on image URL across Google, Yandex, Bing, and TinEye. Extract all matching results and metadata.\",\n    \"deep_web_search\"\
    : \"Search deep web forums and paste sites for mentions of keyword. Use Tor proxy and handle onion sites.\",\n    \"public_records_lookup\"\
    : \"Scrape public records for person name in specified state/country. Include court records, property, and business registrations.\"\
    ,\n    \"email_harvest_domain\": \"Harvest all email addresses associated with domain name. Search website, social media,\
    \ and public databases.\",\n    \"phone_lookup_osint\": \"Perform comprehensive phone number lookup including carrier,\
    \ location, social media profiles, and public records.\",\n    \"bulk_social_export\": \"Export all scraped social media\
    \ data to JSON, CSV, and SQLite formats with full metadata and timestamps.\"\n}\n\nINTEGRATION_POINTS = {\n    \"api_endpoints\"\
    : {\n        \"scrape_twitter\": \"/api/scrape/twitter\",\n        \"scrape_instagram\": \"/api/scrape/instagram\",\n\
    \        \"scrape_facebook\": \"/api/scrape/facebook\",\n        \"scrape_tiktok\": \"/api/scrape/tiktok\",\n        \"\
    scrape_reddit\": \"/api/scrape/reddit\",\n        \"scrape_linkedin\": \"/api/scrape/linkedin\",\n        \"scrape_youtube\"\
    : \"/api/scrape/youtube\",\n        \"username_enum\": \"/api/enum/username\",\n        \"wayback_recovery\": \"/api/wayback/recover\"\
    ,\n        \"reverse_image\": \"/api/image/reverse\",\n        \"deep_web\": \"/api/deepweb/search\",\n        \"public_records\"\
    : \"/api/records/search\",\n        \"email_harvest\": \"/api/email/harvest\",\n        \"phone_lookup\": \"/api/phone/lookup\"\
    ,\n        \"bulk_export\": \"/api/export/bulk\"\n    },\n    \"webhooks\": {\n        \"scrape_complete\": \"https://your-webhook.com/scrape-complete\"\
    ,\n        \"enum_complete\": \"https://your-webhook.com/enum-complete\",\n        \"export_ready\": \"https://your-webhook.com/export-ready\"\
    \n    },\n    \"databases\": {\n        \"scraped_data\": \"sqlite:///scraped_data.db\",\n        \"enum_results\": \"\
    sqlite:///enum_results.db\",\n        \"export_files\": \"./exports/\"\n    }\n}\n\n@dataclass\nclass ScrapedItem:\n \
    \   url: str\n    platform: str\n    username: Optional[str] = None\n    content: Optional[str] = None\n    media_urls:\
    \ List[str] = None\n    metadata: Dict[str, Any] = None\n    timestamp: datetime = None\n    \n    def __post_init__(self):\n\
    \        if self.timestamp is None:\n            self.timestamp = datetime.now()\n        if self.media_urls is None:\n\
    \            self.media_urls = []\n        if self.metadata is None:\n            self.metadata = {}\n\nclass ProxyRotator:\n\
    \    def __init__(self, proxy_list: List[str] = None):\n        self.proxies = proxy_list or [\n            \"http://proxy1:8080\"\
    ,\n            \"http://proxy2:8080\",\n            \"http://proxy3:8080\"\n        ]\n        self.current = 0\n    \n\
    \    def get_proxy(self) -> str:\n        proxy = self.proxies[self.current]\n        self.current = (self.current + 1)\
    \ % len(self.proxies)\n        return proxy\n\nclass AntiDetection:\n    @staticmethod\n    def get_random_user_agent()\
    \ -> str:\n        agents = [\n            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like\
    \ Gecko) Chrome/120.0.0.0 Safari/537.36\",\n            \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36\
    \ (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n            \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36\
    \ (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n        ]\n        return random.choice(agents)\n    \n    @staticmethod\n\
    \    def get_random_delay() -> float:\n        return random.uniform(1, 3)\n\nclass RateLimiter:\n    def __init__(self,\
    \ max_requests: int = 100, window_seconds: int = 60):\n        self.max_requests = max_requests\n        self.window_seconds\
    \ = window_seconds\n        self.requests = []\n    \n    def wait_if_needed(self):\n        now = time.time()\n     \
    \   self.requests = [req_time for req_time in self.requests if now - req_time < self.window_seconds]\n        \n     \
    \   if len(self.requests) >= self.max_requests:\n            sleep_time = self.window_seconds - (now - self.requests[0])\n\
    \            if sleep_time > 0:\n                time.sleep(sleep_time)\n        \n        self.requests.append(now)\n\
    \nclass CAPTCHASolver:\n    @staticmethod\n    def solve_captcha(image_path: str) -> str:\n        # Placeholder for CAPTCHA\
    \ solving service integration\n        return \"SOLVED_TEXT\"\n\nclass DataExporter:\n    @staticmethod\n    def to_json(data:\
    \ List[Dict], filename: str):\n        with open(filename, 'w', encoding='utf-8') as f:\n            json.dump(data, f,\
    \ indent=2, default=str)\n    \n    @staticmethod\n    def to_csv(data: List[Dict], filename: str):\n        if not data:\n\
    \            return\n        keys = data[0].keys()\n        with open(filename, 'w', newline='', encoding='utf-8') as\
    \ f:\n            writer = csv.DictWriter(f, fieldnames=keys)\n            writer.writeheader()\n            writer.writerows(data)\n\
    \    \n    @staticmethod\n    def to_sqlite(data: List[Dict], db_path: str, table_name: str):\n        conn = sqlite3.connect(db_path)\n\
    \        if data:\n            keys = data[0].keys()\n            columns = ', '.join([f'{k} TEXT' for k in keys])\n \
    \           conn.execute(f'CREATE TABLE IF NOT EXISTS {table_name} ({columns})')\n            \n            placeholders\
    \ = ', '.join(['?' for _ in keys])\n            conn.executemany(\n                f'INSERT INTO {table_name} ({\", \"\
    .join(keys)}) VALUES ({placeholders})',\n                [[str(item.get(k, '')) for k in keys] for item in data]\n   \
    \         )\n            conn.commit()\n        conn.close()\n\nclass UsernameEnumerator:\n    PLATFORMS = [\n       \
    \ \"twitter.com\", \"instagram.com\", \"facebook.com\", \"tiktok.com\", \"linkedin.com\",\n        \"youtube.com\", \"\
    reddit.com\", \"github.com\", \"pinterest.com\", \"tumblr.com\",\n        \"snapchat.com\", \"whatsapp.com\", \"telegram.org\"\
    , \"discord.com\", \"spotify.com\",\n        \"medium.com\", \"quora.com\", \"stackexchange.com\", \"stackoverflow.com\"\
    ,\n        \"deviantart.com\", \"behance.net\", \"dribbble.com\", \"flickr.com\", \"vimeo.com\",\n        \"soundcloud.com\"\
    , \"bandcamp.com\", \"myspace.com\", \"foursquare.com\"\n    ]\n    \n    @staticmethod\n    async def check_username(username:\
    \ str, platform: str) -> Dict[str, Any]:\n        url = f\"https://{platform}/{username}\"\n        try:\n           \
    \ async with aiohttp.ClientSession() as session:\n                async with session.get(url, timeout=10) as response:\n\
    \                    return {\n                        \"platform\": platform,\n                        \"username\":\
    \ username,\n                        \"available\": response.status == 404,\n                        \"url\": url,\n \
    \                       \"status_code\": response.status\n                    }\n        except Exception as e:\n    \
    \        return {\n                \"platform\": platform,\n                \"username\": username,\n                \"\
    available\": None,\n                \"error\": str(e)\n            }\n    \n    @classmethod\n    async def enumerate_all(cls,\
    \ username: str) -> List[Dict[str, Any]]:\n        tasks = [cls.check_username(username, platform) for platform in cls.PLATFORMS]\n\
    \        return await asyncio.gather(*tasks)\n\nclass WaybackMachine:\n    @staticmethod\n    def get_snapshots(url: str,\
    \ from_date: str = None, to_date: str = None) -> List[Dict[str, Any]]:\n        api_url = f\"http://web.archive.org/cdx/search/cdx\"\
    \n        params = {\n            \"url\": url,\n            \"output\": \"json\",\n            \"fl\": \"timestamp,original\"\
    \n        }\n        if from_date:\n            params[\"from\"] = from_date\n        if to_date:\n            params[\"\
    to\"] = to_date\n        \n        response = requests.get(api_url, params=params)\n        if response.status_code ==\
    \ 200:\n            data = response.json()\n            return [{\"timestamp\": item[0], \"url\": item[1]} for item in\
    \ data[1:]]\n        return []\n\nclass ReverseImageSearch:\n    @staticmethod\n    def google_reverse_search(image_url:\
    \ str) -> List[Dict[str, str]]:\n        # Placeholder for Google reverse image search\n        return [{\"source\": \"\
    google\", \"url\": \"https://example.com/result1\"}]\n    \n    @staticmethod\n    def yandex_reverse_search(image_url:\
    \ str) -> List[Dict[str, str]]:\n        # Placeholder for Yandex reverse image search\n        return [{\"source\": \"\
    yandex\", \"url\": \"https://example.com/result2\"}]\n\nclass SocialMediaScraper:\n    def __init__(self):\n        self.proxy_rotator\
    \ = ProxyRotator()\n        self.rate_limiter = RateLimiter()\n        self.anti_detection = AntiDetection()\n    \n \
    \   async def scrape_twitter(self, username: str) -> List[ScrapedItem]:\n        items = []\n        url = f\"https://twitter.com/{username}\"\
    \n        \n        async with async_playwright() as p:\n            browser = await p.chromium.launch(headless=True)\n\
    \            context = await browser.new_context(\n                user_agent=self.anti_detection.get_random_user_agent()\n\
    \            )\n            page = await context.new_page()\n            \n            await page.goto(url)\n        \
    \    await page.wait_for_timeout(5000)\n            \n            tweets = await page.query_selector_all('[data-testid=\"\
    tweet\"]')\n            for tweet in tweets:\n                content = await tweet.text_content()\n                items.append(ScrapedItem(\n\
    \                    url=url,\n                    platform=\"twitter\",\n                    username=username,\n   \
    \                 content=content\n                ))\n            \n            await browser.close()\n        return\
    \ items\n    \n    async def scrape_instagram(self, username: str) -> List[ScrapedItem]:\n        items = []\n       \
    \ url = f\"https://instagram.com/{username}\"\n        \n        async with async_playwright() as p:\n            browser\
    \ = await p.chromium.launch(headless=True)\n            page = await browser.new_page()\n            await page.goto(url)\n\
    \            await page.wait_for_timeout(5000)\n            \n            posts = await page.query_selector_all('article\
    \ a')\n            for post in posts[:10]:\n                post_url = await post.get_attribute('href')\n            \
    \    if post_url:\n                    items.append(ScrapedItem(\n                        url=urljoin(url, post_url),\n\
    \                        platform=\"instagram\",\n                        username=username\n                    ))\n\
    \            \n            await browser.close()\n        return items\n    \n    def scrape_reddit(self, subreddit: str,\
    \ limit: int = 100) -> List[ScrapedItem]:\n        items = []\n        url = f\"https://reddit.com/r/{subreddit}/hot.json\"\
    \n        headers = {\"User-Agent\": self.anti_detection.get_random_user_agent()}\n        \n        response = requests.get(url,\
    \ headers=headers, params={\"limit\": limit})\n        if response.status_code == 200:\n            data = response.json()\n\
    \            for post in data[\"data\"][\"children\"]:\n                items.append(ScrapedItem(\n                  \
    \  url=f\"https://reddit.com{post['data']['permalink']}\",\n                    platform=\"reddit\",\n               \
    \     username=post[\"data\"][\"author\"],\n                    content=post[\"data\"][\"title\"]\n                ))\n\
    \        return items\n\nclass SkillEngine:\n    def __init__(self):\n        self.scraper = SocialMediaScraper()\n  \
    \      self.exporter = DataExporter()\n        self.enum = UsernameEnumerator()\n        self.wayback = WaybackMachine()\n\
    \        self.reverse_image = ReverseImageSearch()\n    \n    async def execute(self, prompt: str, params: Dict[str, Any])\
    \ -> Dict[str, Any]:\n        if prompt == \"scrape_twitter\":\n            return {\"data\": await self.scraper.scrape_twitter(params[\"\
    username\"])}\n        elif prompt == \"scrape_instagram\":\n            return {\"data\": await self.scraper.scrape_instagram(params[\"\
    username\"])}\n        elif prompt == \"scrape_reddit\":\n            return {\"data\": self.scraper.scrape_reddit(params[\"\
    subreddit\"], params.get(\"limit\", 100))}\n        elif prompt == \"username_enum\":\n            return {\"data\": await\
    \ self.enum.enumerate_all(params[\"username\"])}\n        elif prompt == \"wayback_recovery\":\n            return {\"\
    data\": self.wayback.get_snapshots(params[\"url\"])}\n        elif prompt == \"reverse_image\":\n            return {\n\
    \                \"google\": self.reverse_image.google_reverse_search(params[\"image_url\"]),\n                \"yandex\"\
    : self.reverse_image.yandex_reverse_search(params[\"image_url\"])\n            }\n        elif prompt == \"bulk_export\"\
    :\n            data = params[\"data\"]\n            format_type = params.get(\"format\", \"json\")\n            filename\
    \ = params.get(\"filename\", \"export\")\n            \n            if format_type == \"json\":\n                self.exporter.to_json(data,\
    \ f\"{filename}.json\")\n            elif format_type == \"csv\":\n                self.exporter.to_csv(data, f\"{filename}.csv\"\
    )\n            elif format_type == \"sqlite\":\n                self.exporter.to_sqlite(data, f\"{filename}.db\", \"scraped_data\"\
    )\n            \n            return {\"status\": \"exported\", \"filename\": filename}\n        \n        return {\"error\"\
    : \"Unknown prompt\"}\n\nclass TestUnrestrictedWebScraper(unittest.TestCase):\n    def setUp(self):\n        self.engine\
    \ = SkillEngine()\n    \n    def test_proxy_rotator(self):\n        rotator = ProxyRotator([\"http://proxy1:8080\", \"\
    http://proxy2:8080\"])\n        self.assertIn(rotator.get_proxy(), [\"http://proxy1:8080\", \"http://proxy2:8080\"])\n\
    \    \n    def test_anti_detection_user_agent(self):\n        agent = AntiDetection.get_random_user_agent()\n        self.assertTrue(agent.startswith(\"\
    Mozilla/\"))\n    \n    def test_rate_limiter(self):\n        limiter = RateLimiter(max_requests=2, window_seconds=1)\n\
    \        start = time.time()\n        limiter.wait_if_needed()\n        limiter.wait_if_needed()\n        limiter.wait_if_needed()\n\
    \        self.assertTrue(time.time() - start >= 1)\n    \n    def test_data_exporter_json(self):\n        data = [{\"\
    test\": \"value\"}]\n        DataExporter.to_json(data, \"test.json\")\n        self.assertTrue(os.path.exists(\"test.json\"\
    ))\n        os.remove(\"test.json\")\n    \n    def test_data_exporter_csv(self):\n        data = [{\"col1\": \"val1\"\
    , \"col2\": \"val2\"}]\n        DataExporter.to_csv(data, \"test.csv\")\n        self.assertTrue(os.path.exists(\"test.csv\"\
    ))\n        os.remove(\"test.csv\")\n    \n    def test_data_exporter_sqlite(self):\n        data = [{\"col1\": \"val1\"\
    , \"col2\": \"val2\"}]\n        DataExporter.to_sqlite(data, \"test.db\", \"test_table\")\n        conn = sqlite3.connect(\"\
    test.db\")\n        cursor = conn.execute(\"SELECT * FROM test_table\")\n        self.assertEqual(len(cursor.fetchall()),\
    \ 1)\n        conn.close()\n        os.remove(\"test.db\")\n    \n    def test_wayback_snapshots(self):\n        with\
    \ patch('requests.get') as mock_get:\n            mock_get.return_value.status_code = 200\n            mock_get.return_value.json.return_value\
    \ = [\n                [\"timestamp\", \"original\"],\n                [\"20230101000000\", \"http://example.com\"]\n\
    \            ]\n            results = WaybackMachine.get_snapshots(\"http://example.com\")\n            self.assertEqual(len(results),\
    \ 1)\n            self.assertEqual(results[0][\"timestamp\"], \"20230101000000\")\n    \n    def test_username_enumerator_platforms(self):\n\
    \        self.assertGreater(len(UsernameEnumerator.PLATFORMS), 20)\n    \n    async def test_username_check(self):\n \
    \       with patch('aiohttp.ClientSession.get') as mock_get:\n            mock_get.return_value.__aenter__.return_value.status\
    \ = 200\n            result = await UsernameEnumerator.check_username(\"test\", \"twitter.com\")\n            self.assertEqual(result[\"\
    username\"], \"test\")\n    \n    def test_scraped_item_creation(self):\n        item = ScrapedItem(url=\"http://test.com\"\
    , platform=\"test\")\n        self.assertEqual(item.platform, \"test\")\n        self.assertIsNotNone(item.timestamp)\n\
    \    \n    def test_skill_engine_unknown_prompt(self):\n        result = asyncio.run(self.engine.execute(\"unknown\",\
    \ {}))\n        self.assertIn(\"error\", result)\n    \n    def test_reverse_image_search(self):\n        results = ReverseImageSearch.google_reverse_search(\"\
    http://test.jpg\")\n        self.assertIsInstance(results, list)\n\nif __name__ == \"__main__\":\n    unittest.main()"
examples:
- description: Load and use the UnrestrictedWebScraper skill
  usage: 'from revvel_skills import load_skill

    skill = load_skill(''web_scraper'')

    result = skill.execute(params)'
schema_version: '1.0'
